{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AaX9bi035a8e",
        "VQaBSU9I5dFE",
        "_ZUB_EjA69kk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise information"
      ],
      "metadata": {
        "id": "gZuZD8DeNLU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/learn/audio-course/en/chapter2/hands_on"
      ],
      "metadata": {
        "id": "F2cZrOOQNPyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Use ðŸ¤— Datasets to load the train split of the facebook/voxpopuli dataset in language of your choice in streaming mode.\n",
        "2. Get the third example from the train part of the dataset and explore it. Given the features that this example has, what kinds of audio tasks can you use this dataset for?\n",
        "3. Plot this exampleâ€™s waveform and spectrogram.\n",
        "4. Go to ðŸ¤— Hub, explore pretrained models and find one that can be used for automatic speech recognition for the language that you have picked earlier. Instantiate a corresponding pipeline with the model you found, and transcribe the example.\n",
        "5. Compare the transcription that you get from the pipeline to the transcription provided in the example.\n"
      ],
      "metadata": {
        "id": "l96k4ZbiM-7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Use ðŸ¤— Datasets to load the train split of the facebook/voxpopuli dataset in language of your choice in streaming mode."
      ],
      "metadata": {
        "id": "AaX9bi035a8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UbGlC2DnM3eZ"
      },
      "outputs": [],
      "source": [
        "pip install datasets[audio]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yp6TS6YWOUwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Audio\n",
        "\n",
        "voxpopuli = load_dataset(\"facebook/voxpopuli\", \"fi\", split = \"train\", streaming = True) # load only train, english doesn't have train for some reason, just test, sampling rate is already 16_000"
      ],
      "metadata": {
        "id": "LeEFUCr8NZ2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Get the third example from the train part of the dataset and explore it. Given the features that this example has, what kinds of audio tasks can you use this dataset for?"
      ],
      "metadata": {
        "id": "VQaBSU9I5dFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first3 = list(voxpopuli.take(3)) # first 3 examples\n",
        "first3"
      ],
      "metadata": {
        "id": "fnNfcsw95P_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example3 = list(first3)[-1] # acess last element aka the third example\n",
        "example3"
      ],
      "metadata": {
        "id": "pB8HF7XGQnIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = voxpopuli.features[\"language\"].int2str\n",
        "id2label(example3[\"language\"]) # checking that language: 10 is finnish"
      ],
      "metadata": {
        "id": "MCPJjFwz53BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the feautures this example has, this dataset can be used for the following audio tasks:\n",
        "\n",
        "1. Audio classification between male and female and accent recognition\n",
        "2. Speaker diarization as we have the speaker_id\n",
        "3. Automatic speech recognition"
      ],
      "metadata": {
        "id": "Tdmv_9FV6Jnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Plot this exampleâ€™s waveform and spectrogram"
      ],
      "metadata": {
        "id": "_ZUB_EjA69kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'audio': {'path': 'train_part_0/20190327-0900-PLENARY-fi_20190327-18:47:03_12.wav',\n",
        "   'array': array([2.74658203e-04, 2.44140625e-04, 3.05175781e-05, ...,\n",
        "          6.71386719e-04, 1.80053711e-03, 2.22778320e-03]),\n",
        "   'sampling_rate': 16000},"
      ],
      "metadata": {
        "id": "H_bv7ehS9BSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array = example3[\"audio\"][\"array\"]\n",
        "sampling_rate = example3[\"audio\"][\"sampling_rate\"]"
      ],
      "metadata": {
        "id": "1wd5G_Hm9EmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Waveform"
      ],
      "metadata": {
        "id": "4aZOPBPE9VKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "plt.title(\"Waveform of example 3\")\n",
        "librosa.display.waveshow(array, sr=sampling_rate)"
      ],
      "metadata": {
        "id": "DX2OBSWF7MwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectrogram"
      ],
      "metadata": {
        "id": "xWxbdF2t9gX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "D = librosa.stft(array)\n",
        "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "plt.title(\"Spectrogram of example 3\")\n",
        "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "vOgR8mmS9a41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Go to ðŸ¤— Hub, explore pretrained models and find one that can be used for automatic speech recognition for the language that you have picked earlier. Instantiate a corresponding pipeline with the model you found, and transcribe the example."
      ],
      "metadata": {
        "id": "hBk20OOm9ta9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example3['normalized_text'] # provided transcription"
      ],
      "metadata": {
        "id": "EyaFUMH494LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "asr = pipeline(\"automatic-speech-recognition\",model=\"Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm\")\n",
        "asr(example3[\"audio\"][\"array\"])"
      ],
      "metadata": {
        "id": "1XHUz7-Z_Eju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Compare the transcription that you get from the pipeline to the transcription provided in the example."
      ],
      "metadata": {
        "id": "4ZZ6LRKc92Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is the same text"
      ],
      "metadata": {
        "id": "fFzBbj2W_w2s"
      }
    }
  ]
}